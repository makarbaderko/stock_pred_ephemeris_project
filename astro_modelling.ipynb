{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Date', 'id', 'date_y', 'sun', 'moon', 'mercury', 'venus',\n",
       "       'mars', 'jupiter', 'saturn', 'uran', 'neptun', 'pluton', 'Open', 'High',\n",
       "       'Low', 'Close', 'Adj Close', 'Volume'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>id</th>\n",
       "      <th>date_y</th>\n",
       "      <th>sun</th>\n",
       "      <th>moon</th>\n",
       "      <th>mercury</th>\n",
       "      <th>venus</th>\n",
       "      <th>mars</th>\n",
       "      <th>jupiter</th>\n",
       "      <th>saturn</th>\n",
       "      <th>uran</th>\n",
       "      <th>neptun</th>\n",
       "      <th>pluton</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981-01-05</td>\n",
       "      <td>18633</td>\n",
       "      <td>2444610.0</td>\n",
       "      <td>285.076029</td>\n",
       "      <td>275.478232</td>\n",
       "      <td>288.132610</td>\n",
       "      <td>262.765480</td>\n",
       "      <td>304.367966</td>\n",
       "      <td>189.805239</td>\n",
       "      <td>189.627260</td>\n",
       "      <td>238.630702</td>\n",
       "      <td>263.214863</td>\n",
       "      <td>204.212438</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.150670</td>\n",
       "      <td>0.150670</td>\n",
       "      <td>0.118667</td>\n",
       "      <td>35728000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1981-01-19</td>\n",
       "      <td>18647</td>\n",
       "      <td>2444624.0</td>\n",
       "      <td>299.338699</td>\n",
       "      <td>109.020889</td>\n",
       "      <td>311.518415</td>\n",
       "      <td>280.280424</td>\n",
       "      <td>315.407030</td>\n",
       "      <td>190.344361</td>\n",
       "      <td>189.783566</td>\n",
       "      <td>239.232013</td>\n",
       "      <td>263.691294</td>\n",
       "      <td>204.330776</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.147321</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.115590</td>\n",
       "      <td>41574400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1981-01-20</td>\n",
       "      <td>18648</td>\n",
       "      <td>2444625.0</td>\n",
       "      <td>300.356085</td>\n",
       "      <td>122.615366</td>\n",
       "      <td>313.191977</td>\n",
       "      <td>281.531811</td>\n",
       "      <td>316.197121</td>\n",
       "      <td>190.359651</td>\n",
       "      <td>189.781234</td>\n",
       "      <td>239.269810</td>\n",
       "      <td>263.723289</td>\n",
       "      <td>204.334748</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.112074</td>\n",
       "      <td>30083200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1981-01-21</td>\n",
       "      <td>18649</td>\n",
       "      <td>2444626.0</td>\n",
       "      <td>301.373281</td>\n",
       "      <td>135.918247</td>\n",
       "      <td>314.855576</td>\n",
       "      <td>282.783233</td>\n",
       "      <td>316.987353</td>\n",
       "      <td>190.371775</td>\n",
       "      <td>189.777096</td>\n",
       "      <td>239.306862</td>\n",
       "      <td>263.754959</td>\n",
       "      <td>204.338106</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.114272</td>\n",
       "      <td>15904000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1981-01-22</td>\n",
       "      <td>18650</td>\n",
       "      <td>2444627.0</td>\n",
       "      <td>302.390292</td>\n",
       "      <td>148.911797</td>\n",
       "      <td>316.505449</td>\n",
       "      <td>284.034693</td>\n",
       "      <td>317.777719</td>\n",
       "      <td>190.380723</td>\n",
       "      <td>189.771153</td>\n",
       "      <td>239.343159</td>\n",
       "      <td>263.786297</td>\n",
       "      <td>204.340851</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.147879</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.115590</td>\n",
       "      <td>35548800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7807</th>\n",
       "      <td>2011-12-23</td>\n",
       "      <td>29942</td>\n",
       "      <td>2455919.0</td>\n",
       "      <td>271.294796</td>\n",
       "      <td>254.520833</td>\n",
       "      <td>249.550741</td>\n",
       "      <td>303.368064</td>\n",
       "      <td>167.772252</td>\n",
       "      <td>30.374792</td>\n",
       "      <td>207.702916</td>\n",
       "      <td>0.716550</td>\n",
       "      <td>328.671539</td>\n",
       "      <td>277.019690</td>\n",
       "      <td>14.274643</td>\n",
       "      <td>14.413929</td>\n",
       "      <td>14.267500</td>\n",
       "      <td>14.404643</td>\n",
       "      <td>12.426182</td>\n",
       "      <td>269399200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7808</th>\n",
       "      <td>2011-12-27</td>\n",
       "      <td>29946</td>\n",
       "      <td>2455923.0</td>\n",
       "      <td>275.371541</td>\n",
       "      <td>310.545496</td>\n",
       "      <td>254.069573</td>\n",
       "      <td>308.293249</td>\n",
       "      <td>168.943557</td>\n",
       "      <td>30.368860</td>\n",
       "      <td>207.991687</td>\n",
       "      <td>0.768815</td>\n",
       "      <td>328.769952</td>\n",
       "      <td>277.162639</td>\n",
       "      <td>14.396428</td>\n",
       "      <td>14.610357</td>\n",
       "      <td>14.393572</td>\n",
       "      <td>14.518929</td>\n",
       "      <td>12.524765</td>\n",
       "      <td>265076000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7809</th>\n",
       "      <td>2011-12-28</td>\n",
       "      <td>29947</td>\n",
       "      <td>2455924.0</td>\n",
       "      <td>276.390843</td>\n",
       "      <td>323.695976</td>\n",
       "      <td>255.300657</td>\n",
       "      <td>309.523140</td>\n",
       "      <td>169.217487</td>\n",
       "      <td>30.376122</td>\n",
       "      <td>208.060364</td>\n",
       "      <td>0.783974</td>\n",
       "      <td>328.795638</td>\n",
       "      <td>277.198397</td>\n",
       "      <td>14.531786</td>\n",
       "      <td>14.580358</td>\n",
       "      <td>14.333571</td>\n",
       "      <td>14.380000</td>\n",
       "      <td>12.404922</td>\n",
       "      <td>228662000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7810</th>\n",
       "      <td>2011-12-29</td>\n",
       "      <td>29948</td>\n",
       "      <td>2455925.0</td>\n",
       "      <td>277.410143</td>\n",
       "      <td>336.477531</td>\n",
       "      <td>256.563080</td>\n",
       "      <td>310.752405</td>\n",
       "      <td>169.483566</td>\n",
       "      <td>30.386872</td>\n",
       "      <td>208.127605</td>\n",
       "      <td>0.799964</td>\n",
       "      <td>328.821745</td>\n",
       "      <td>277.234148</td>\n",
       "      <td>14.407143</td>\n",
       "      <td>14.487500</td>\n",
       "      <td>14.303928</td>\n",
       "      <td>14.468572</td>\n",
       "      <td>12.481326</td>\n",
       "      <td>215978000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7811</th>\n",
       "      <td>2011-12-30</td>\n",
       "      <td>29949</td>\n",
       "      <td>2455926.0</td>\n",
       "      <td>278.429428</td>\n",
       "      <td>348.927587</td>\n",
       "      <td>257.853290</td>\n",
       "      <td>311.981016</td>\n",
       "      <td>169.741649</td>\n",
       "      <td>30.401103</td>\n",
       "      <td>208.193399</td>\n",
       "      <td>0.816785</td>\n",
       "      <td>328.848270</td>\n",
       "      <td>277.269888</td>\n",
       "      <td>14.411072</td>\n",
       "      <td>14.510000</td>\n",
       "      <td>14.410357</td>\n",
       "      <td>14.464286</td>\n",
       "      <td>12.477631</td>\n",
       "      <td>179662000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7812 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date     id     date_y         sun        moon     mercury  \\\n",
       "0     1981-01-05  18633  2444610.0  285.076029  275.478232  288.132610   \n",
       "1     1981-01-19  18647  2444624.0  299.338699  109.020889  311.518415   \n",
       "2     1981-01-20  18648  2444625.0  300.356085  122.615366  313.191977   \n",
       "3     1981-01-21  18649  2444626.0  301.373281  135.918247  314.855576   \n",
       "4     1981-01-22  18650  2444627.0  302.390292  148.911797  316.505449   \n",
       "...          ...    ...        ...         ...         ...         ...   \n",
       "7807  2011-12-23  29942  2455919.0  271.294796  254.520833  249.550741   \n",
       "7808  2011-12-27  29946  2455923.0  275.371541  310.545496  254.069573   \n",
       "7809  2011-12-28  29947  2455924.0  276.390843  323.695976  255.300657   \n",
       "7810  2011-12-29  29948  2455925.0  277.410143  336.477531  256.563080   \n",
       "7811  2011-12-30  29949  2455926.0  278.429428  348.927587  257.853290   \n",
       "\n",
       "           venus        mars     jupiter      saturn        uran      neptun  \\\n",
       "0     262.765480  304.367966  189.805239  189.627260  238.630702  263.214863   \n",
       "1     280.280424  315.407030  190.344361  189.783566  239.232013  263.691294   \n",
       "2     281.531811  316.197121  190.359651  189.781234  239.269810  263.723289   \n",
       "3     282.783233  316.987353  190.371775  189.777096  239.306862  263.754959   \n",
       "4     284.034693  317.777719  190.380723  189.771153  239.343159  263.786297   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "7807  303.368064  167.772252   30.374792  207.702916    0.716550  328.671539   \n",
       "7808  308.293249  168.943557   30.368860  207.991687    0.768815  328.769952   \n",
       "7809  309.523140  169.217487   30.376122  208.060364    0.783974  328.795638   \n",
       "7810  310.752405  169.483566   30.386872  208.127605    0.799964  328.821745   \n",
       "7811  311.981016  169.741649   30.401103  208.193399    0.816785  328.848270   \n",
       "\n",
       "          pluton       Open       High        Low      Close  Adj Close  \\\n",
       "0     204.212438   0.151228   0.151228   0.150670   0.150670   0.118667   \n",
       "1     204.330776   0.146763   0.147321   0.146763   0.146763   0.115590   \n",
       "2     204.334748   0.142857   0.142857   0.142299   0.142299   0.112074   \n",
       "3     204.338106   0.145089   0.146205   0.145089   0.145089   0.114272   \n",
       "4     204.340851   0.146763   0.147879   0.146763   0.146763   0.115590   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "7807  277.019690  14.274643  14.413929  14.267500  14.404643  12.426182   \n",
       "7808  277.162639  14.396428  14.610357  14.393572  14.518929  12.524765   \n",
       "7809  277.198397  14.531786  14.580358  14.333571  14.380000  12.404922   \n",
       "7810  277.234148  14.407143  14.487500  14.303928  14.468572  12.481326   \n",
       "7811  277.269888  14.411072  14.510000  14.410357  14.464286  12.477631   \n",
       "\n",
       "           Volume  \n",
       "0      35728000.0  \n",
       "1      41574400.0  \n",
       "2      30083200.0  \n",
       "3      15904000.0  \n",
       "4      35548800.0  \n",
       "...           ...  \n",
       "7807  269399200.0  \n",
       "7808  265076000.0  \n",
       "7809  228662000.0  \n",
       "7810  215978000.0  \n",
       "7811  179662000.0  \n",
       "\n",
       "[7812 rows x 19 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr.replace(1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_y</th>\n",
       "      <th>sun</th>\n",
       "      <th>moon</th>\n",
       "      <th>mercury</th>\n",
       "      <th>venus</th>\n",
       "      <th>mars</th>\n",
       "      <th>jupiter</th>\n",
       "      <th>saturn</th>\n",
       "      <th>uran</th>\n",
       "      <th>neptun</th>\n",
       "      <th>pluton</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>-0.010786</td>\n",
       "      <td>-0.008692</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.020490</td>\n",
       "      <td>-0.454905</td>\n",
       "      <td>0.263516</td>\n",
       "      <td>0.997613</td>\n",
       "      <td>0.997482</td>\n",
       "      <td>0.653649</td>\n",
       "      <td>0.654768</td>\n",
       "      <td>0.652486</td>\n",
       "      <td>0.653655</td>\n",
       "      <td>0.655178</td>\n",
       "      <td>0.561364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_y</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>-0.010786</td>\n",
       "      <td>-0.008692</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.020490</td>\n",
       "      <td>-0.454905</td>\n",
       "      <td>0.263516</td>\n",
       "      <td>0.997613</td>\n",
       "      <td>0.997482</td>\n",
       "      <td>0.653649</td>\n",
       "      <td>0.654768</td>\n",
       "      <td>0.652486</td>\n",
       "      <td>0.653655</td>\n",
       "      <td>0.655178</td>\n",
       "      <td>0.561364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sun</th>\n",
       "      <td>-0.000862</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.770897</td>\n",
       "      <td>0.580655</td>\n",
       "      <td>0.215316</td>\n",
       "      <td>0.061775</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.065289</td>\n",
       "      <td>-0.023778</td>\n",
       "      <td>0.020724</td>\n",
       "      <td>-0.004941</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.004932</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.005014</td>\n",
       "      <td>0.052158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moon</th>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010304</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>-0.002698</td>\n",
       "      <td>-0.006018</td>\n",
       "      <td>-0.004767</td>\n",
       "      <td>-0.004758</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>0.008744</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>0.009105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mercury</th>\n",
       "      <td>-0.010786</td>\n",
       "      <td>-0.010786</td>\n",
       "      <td>0.770897</td>\n",
       "      <td>-0.010304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537485</td>\n",
       "      <td>0.207486</td>\n",
       "      <td>0.031173</td>\n",
       "      <td>0.039653</td>\n",
       "      <td>0.081003</td>\n",
       "      <td>-0.030603</td>\n",
       "      <td>0.012023</td>\n",
       "      <td>-0.023097</td>\n",
       "      <td>-0.022910</td>\n",
       "      <td>-0.023097</td>\n",
       "      <td>-0.023106</td>\n",
       "      <td>-0.023047</td>\n",
       "      <td>0.060822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>venus</th>\n",
       "      <td>-0.008692</td>\n",
       "      <td>-0.008692</td>\n",
       "      <td>0.580655</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.537485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200847</td>\n",
       "      <td>0.035163</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>-0.011956</td>\n",
       "      <td>-0.030336</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.022478</td>\n",
       "      <td>0.022368</td>\n",
       "      <td>0.022303</td>\n",
       "      <td>0.022326</td>\n",
       "      <td>0.065614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mars</th>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>0.215316</td>\n",
       "      <td>-0.002698</td>\n",
       "      <td>0.207486</td>\n",
       "      <td>0.200847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033713</td>\n",
       "      <td>-0.033534</td>\n",
       "      <td>0.069742</td>\n",
       "      <td>-0.038251</td>\n",
       "      <td>-0.020693</td>\n",
       "      <td>-0.085571</td>\n",
       "      <td>-0.085803</td>\n",
       "      <td>-0.085086</td>\n",
       "      <td>-0.085407</td>\n",
       "      <td>-0.085069</td>\n",
       "      <td>-0.029335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jupiter</th>\n",
       "      <td>-0.020490</td>\n",
       "      <td>-0.020490</td>\n",
       "      <td>0.061775</td>\n",
       "      <td>-0.006018</td>\n",
       "      <td>0.031173</td>\n",
       "      <td>0.035163</td>\n",
       "      <td>0.033713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041830</td>\n",
       "      <td>0.281389</td>\n",
       "      <td>-0.024340</td>\n",
       "      <td>-0.023562</td>\n",
       "      <td>-0.021008</td>\n",
       "      <td>-0.020747</td>\n",
       "      <td>-0.021562</td>\n",
       "      <td>-0.020974</td>\n",
       "      <td>-0.019874</td>\n",
       "      <td>0.099572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saturn</th>\n",
       "      <td>-0.454905</td>\n",
       "      <td>-0.454905</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>-0.004767</td>\n",
       "      <td>0.039653</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>-0.033534</td>\n",
       "      <td>-0.041830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.275889</td>\n",
       "      <td>-0.454462</td>\n",
       "      <td>-0.459298</td>\n",
       "      <td>-0.047606</td>\n",
       "      <td>-0.048266</td>\n",
       "      <td>-0.046910</td>\n",
       "      <td>-0.047589</td>\n",
       "      <td>-0.049660</td>\n",
       "      <td>-0.266256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uran</th>\n",
       "      <td>0.263516</td>\n",
       "      <td>0.263516</td>\n",
       "      <td>0.065289</td>\n",
       "      <td>-0.004758</td>\n",
       "      <td>0.081003</td>\n",
       "      <td>-0.011956</td>\n",
       "      <td>0.069742</td>\n",
       "      <td>0.281389</td>\n",
       "      <td>-0.275889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260524</td>\n",
       "      <td>0.279674</td>\n",
       "      <td>-0.308761</td>\n",
       "      <td>-0.307478</td>\n",
       "      <td>-0.310030</td>\n",
       "      <td>-0.308342</td>\n",
       "      <td>-0.307212</td>\n",
       "      <td>0.240279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neptun</th>\n",
       "      <td>0.997613</td>\n",
       "      <td>0.997613</td>\n",
       "      <td>-0.023778</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>-0.030603</td>\n",
       "      <td>-0.030336</td>\n",
       "      <td>-0.038251</td>\n",
       "      <td>-0.024340</td>\n",
       "      <td>-0.454462</td>\n",
       "      <td>0.260524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997536</td>\n",
       "      <td>0.652471</td>\n",
       "      <td>0.653585</td>\n",
       "      <td>0.651316</td>\n",
       "      <td>0.652486</td>\n",
       "      <td>0.653998</td>\n",
       "      <td>0.559193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pluton</th>\n",
       "      <td>0.997482</td>\n",
       "      <td>0.997482</td>\n",
       "      <td>0.020724</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.012023</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>-0.020693</td>\n",
       "      <td>-0.023562</td>\n",
       "      <td>-0.459298</td>\n",
       "      <td>0.279674</td>\n",
       "      <td>0.997536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.630388</td>\n",
       "      <td>0.631501</td>\n",
       "      <td>0.629232</td>\n",
       "      <td>0.630395</td>\n",
       "      <td>0.631941</td>\n",
       "      <td>0.559440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Open</th>\n",
       "      <td>0.653649</td>\n",
       "      <td>0.653649</td>\n",
       "      <td>-0.004941</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>-0.023097</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>-0.085571</td>\n",
       "      <td>-0.021008</td>\n",
       "      <td>-0.047606</td>\n",
       "      <td>-0.308761</td>\n",
       "      <td>0.652471</td>\n",
       "      <td>0.630388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999870</td>\n",
       "      <td>0.999832</td>\n",
       "      <td>0.999827</td>\n",
       "      <td>0.360963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>0.654768</td>\n",
       "      <td>0.654768</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>0.008744</td>\n",
       "      <td>-0.022910</td>\n",
       "      <td>0.022478</td>\n",
       "      <td>-0.085803</td>\n",
       "      <td>-0.020747</td>\n",
       "      <td>-0.048266</td>\n",
       "      <td>-0.307478</td>\n",
       "      <td>0.653585</td>\n",
       "      <td>0.631501</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999844</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>0.364143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>0.652486</td>\n",
       "      <td>0.652486</td>\n",
       "      <td>-0.004932</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>-0.023097</td>\n",
       "      <td>0.022368</td>\n",
       "      <td>-0.085086</td>\n",
       "      <td>-0.021562</td>\n",
       "      <td>-0.046910</td>\n",
       "      <td>-0.310030</td>\n",
       "      <td>0.651316</td>\n",
       "      <td>0.629232</td>\n",
       "      <td>0.999870</td>\n",
       "      <td>0.999844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.356183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Close</th>\n",
       "      <td>0.653655</td>\n",
       "      <td>0.653655</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>-0.023106</td>\n",
       "      <td>0.022303</td>\n",
       "      <td>-0.085407</td>\n",
       "      <td>-0.020974</td>\n",
       "      <td>-0.047589</td>\n",
       "      <td>-0.308342</td>\n",
       "      <td>0.652486</td>\n",
       "      <td>0.630395</td>\n",
       "      <td>0.999832</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.360351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adj Close</th>\n",
       "      <td>0.655178</td>\n",
       "      <td>0.655178</td>\n",
       "      <td>-0.005014</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>-0.023047</td>\n",
       "      <td>0.022326</td>\n",
       "      <td>-0.085069</td>\n",
       "      <td>-0.019874</td>\n",
       "      <td>-0.049660</td>\n",
       "      <td>-0.307212</td>\n",
       "      <td>0.653998</td>\n",
       "      <td>0.631941</td>\n",
       "      <td>0.999827</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.561364</td>\n",
       "      <td>0.561364</td>\n",
       "      <td>0.052158</td>\n",
       "      <td>0.009105</td>\n",
       "      <td>0.060822</td>\n",
       "      <td>0.065614</td>\n",
       "      <td>-0.029335</td>\n",
       "      <td>0.099572</td>\n",
       "      <td>-0.266256</td>\n",
       "      <td>0.240279</td>\n",
       "      <td>0.559193</td>\n",
       "      <td>0.559440</td>\n",
       "      <td>0.360963</td>\n",
       "      <td>0.364143</td>\n",
       "      <td>0.356183</td>\n",
       "      <td>0.360351</td>\n",
       "      <td>0.361232</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id    date_y       sun      moon   mercury     venus  \\\n",
       "id         0.000000  1.000000 -0.000862  0.001041 -0.010786 -0.008692   \n",
       "date_y     1.000000  0.000000 -0.000862  0.001041 -0.010786 -0.008692   \n",
       "sun       -0.000862 -0.000862  0.000000  0.000363  0.770897  0.580655   \n",
       "moon       0.001041  0.001041  0.000363  0.000000 -0.010304  0.002824   \n",
       "mercury   -0.010786 -0.010786  0.770897 -0.010304  0.000000  0.537485   \n",
       "venus     -0.008692 -0.008692  0.580655  0.002824  0.537485  0.000000   \n",
       "mars      -0.030744 -0.030744  0.215316 -0.002698  0.207486  0.200847   \n",
       "jupiter   -0.020490 -0.020490  0.061775 -0.006018  0.031173  0.035163   \n",
       "saturn    -0.454905 -0.454905  0.025961 -0.004767  0.039653  0.012333   \n",
       "uran       0.263516  0.263516  0.065289 -0.004758  0.081003 -0.011956   \n",
       "neptun     0.997613  0.997613 -0.023778  0.001095 -0.030603 -0.030336   \n",
       "pluton     0.997482  0.997482  0.020724  0.000674  0.012023  0.006897   \n",
       "Open       0.653649  0.653649 -0.004941  0.008630 -0.023097  0.022442   \n",
       "High       0.654768  0.654768 -0.004898  0.008744 -0.022910  0.022478   \n",
       "Low        0.652486  0.652486 -0.004932  0.008333 -0.023097  0.022368   \n",
       "Close      0.653655  0.653655 -0.005058  0.008590 -0.023106  0.022303   \n",
       "Adj Close  0.655178  0.655178 -0.005014  0.008578 -0.023047  0.022326   \n",
       "Volume     0.561364  0.561364  0.052158  0.009105  0.060822  0.065614   \n",
       "\n",
       "               mars   jupiter    saturn      uran    neptun    pluton  \\\n",
       "id        -0.030744 -0.020490 -0.454905  0.263516  0.997613  0.997482   \n",
       "date_y    -0.030744 -0.020490 -0.454905  0.263516  0.997613  0.997482   \n",
       "sun        0.215316  0.061775  0.025961  0.065289 -0.023778  0.020724   \n",
       "moon      -0.002698 -0.006018 -0.004767 -0.004758  0.001095  0.000674   \n",
       "mercury    0.207486  0.031173  0.039653  0.081003 -0.030603  0.012023   \n",
       "venus      0.200847  0.035163  0.012333 -0.011956 -0.030336  0.006897   \n",
       "mars       0.000000  0.033713 -0.033534  0.069742 -0.038251 -0.020693   \n",
       "jupiter    0.033713  0.000000 -0.041830  0.281389 -0.024340 -0.023562   \n",
       "saturn    -0.033534 -0.041830  0.000000 -0.275889 -0.454462 -0.459298   \n",
       "uran       0.069742  0.281389 -0.275889  0.000000  0.260524  0.279674   \n",
       "neptun    -0.038251 -0.024340 -0.454462  0.260524  0.000000  0.997536   \n",
       "pluton    -0.020693 -0.023562 -0.459298  0.279674  0.997536  0.000000   \n",
       "Open      -0.085571 -0.021008 -0.047606 -0.308761  0.652471  0.630388   \n",
       "High      -0.085803 -0.020747 -0.048266 -0.307478  0.653585  0.631501   \n",
       "Low       -0.085086 -0.021562 -0.046910 -0.310030  0.651316  0.629232   \n",
       "Close     -0.085407 -0.020974 -0.047589 -0.308342  0.652486  0.630395   \n",
       "Adj Close -0.085069 -0.019874 -0.049660 -0.307212  0.653998  0.631941   \n",
       "Volume    -0.029335  0.099572 -0.266256  0.240279  0.559193  0.559440   \n",
       "\n",
       "               Open      High       Low     Close  Adj Close    Volume  \n",
       "id         0.653649  0.654768  0.652486  0.653655   0.655178  0.561364  \n",
       "date_y     0.653649  0.654768  0.652486  0.653655   0.655178  0.561364  \n",
       "sun       -0.004941 -0.004898 -0.004932 -0.005058  -0.005014  0.052158  \n",
       "moon       0.008630  0.008744  0.008333  0.008590   0.008578  0.009105  \n",
       "mercury   -0.023097 -0.022910 -0.023097 -0.023106  -0.023047  0.060822  \n",
       "venus      0.022442  0.022478  0.022368  0.022303   0.022326  0.065614  \n",
       "mars      -0.085571 -0.085803 -0.085086 -0.085407  -0.085069 -0.029335  \n",
       "jupiter   -0.021008 -0.020747 -0.021562 -0.020974  -0.019874  0.099572  \n",
       "saturn    -0.047606 -0.048266 -0.046910 -0.047589  -0.049660 -0.266256  \n",
       "uran      -0.308761 -0.307478 -0.310030 -0.308342  -0.307212  0.240279  \n",
       "neptun     0.652471  0.653585  0.651316  0.652486   0.653998  0.559193  \n",
       "pluton     0.630388  0.631501  0.629232  0.630395   0.631941  0.559440  \n",
       "Open       0.000000  0.999932  0.999870  0.999832   0.999827  0.360963  \n",
       "High       0.999932  0.000000  0.999844  0.999918   0.999916  0.364143  \n",
       "Low        0.999870  0.999844  0.000000  0.999896   0.999889  0.356183  \n",
       "Close      0.999832  0.999918  0.999896  0.000000   0.999995  0.360351  \n",
       "Adj Close  0.999827  0.999916  0.999889  0.999995   0.000000  0.361232  \n",
       "Volume     0.360963  0.364143  0.356183  0.360351   0.361232  0.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_corr = corr[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.653649</td>\n",
       "      <td>0.654768</td>\n",
       "      <td>0.652486</td>\n",
       "      <td>0.653655</td>\n",
       "      <td>0.655178</td>\n",
       "      <td>0.561364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_y</th>\n",
       "      <td>0.653649</td>\n",
       "      <td>0.654768</td>\n",
       "      <td>0.652486</td>\n",
       "      <td>0.653655</td>\n",
       "      <td>0.655178</td>\n",
       "      <td>0.561364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sun</th>\n",
       "      <td>-0.004941</td>\n",
       "      <td>-0.004898</td>\n",
       "      <td>-0.004932</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.005014</td>\n",
       "      <td>0.052158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moon</th>\n",
       "      <td>0.008630</td>\n",
       "      <td>0.008744</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>0.009105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mercury</th>\n",
       "      <td>-0.023097</td>\n",
       "      <td>-0.022910</td>\n",
       "      <td>-0.023097</td>\n",
       "      <td>-0.023106</td>\n",
       "      <td>-0.023047</td>\n",
       "      <td>0.060822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>venus</th>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.022478</td>\n",
       "      <td>0.022368</td>\n",
       "      <td>0.022303</td>\n",
       "      <td>0.022326</td>\n",
       "      <td>0.065614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mars</th>\n",
       "      <td>-0.085571</td>\n",
       "      <td>-0.085803</td>\n",
       "      <td>-0.085086</td>\n",
       "      <td>-0.085407</td>\n",
       "      <td>-0.085069</td>\n",
       "      <td>-0.029335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jupiter</th>\n",
       "      <td>-0.021008</td>\n",
       "      <td>-0.020747</td>\n",
       "      <td>-0.021562</td>\n",
       "      <td>-0.020974</td>\n",
       "      <td>-0.019874</td>\n",
       "      <td>0.099572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saturn</th>\n",
       "      <td>-0.047606</td>\n",
       "      <td>-0.048266</td>\n",
       "      <td>-0.046910</td>\n",
       "      <td>-0.047589</td>\n",
       "      <td>-0.049660</td>\n",
       "      <td>-0.266256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uran</th>\n",
       "      <td>-0.308761</td>\n",
       "      <td>-0.307478</td>\n",
       "      <td>-0.310030</td>\n",
       "      <td>-0.308342</td>\n",
       "      <td>-0.307212</td>\n",
       "      <td>0.240279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neptun</th>\n",
       "      <td>0.652471</td>\n",
       "      <td>0.653585</td>\n",
       "      <td>0.651316</td>\n",
       "      <td>0.652486</td>\n",
       "      <td>0.653998</td>\n",
       "      <td>0.559193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pluton</th>\n",
       "      <td>0.630388</td>\n",
       "      <td>0.631501</td>\n",
       "      <td>0.629232</td>\n",
       "      <td>0.630395</td>\n",
       "      <td>0.631941</td>\n",
       "      <td>0.559440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Open</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999870</td>\n",
       "      <td>0.999832</td>\n",
       "      <td>0.999827</td>\n",
       "      <td>0.360963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999844</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>0.364143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>0.999870</td>\n",
       "      <td>0.999844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.356183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Close</th>\n",
       "      <td>0.999832</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.360351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adj Close</th>\n",
       "      <td>0.999827</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.360963</td>\n",
       "      <td>0.364143</td>\n",
       "      <td>0.356183</td>\n",
       "      <td>0.360351</td>\n",
       "      <td>0.361232</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open      High       Low     Close  Adj Close    Volume\n",
       "id         0.653649  0.654768  0.652486  0.653655   0.655178  0.561364\n",
       "date_y     0.653649  0.654768  0.652486  0.653655   0.655178  0.561364\n",
       "sun       -0.004941 -0.004898 -0.004932 -0.005058  -0.005014  0.052158\n",
       "moon       0.008630  0.008744  0.008333  0.008590   0.008578  0.009105\n",
       "mercury   -0.023097 -0.022910 -0.023097 -0.023106  -0.023047  0.060822\n",
       "venus      0.022442  0.022478  0.022368  0.022303   0.022326  0.065614\n",
       "mars      -0.085571 -0.085803 -0.085086 -0.085407  -0.085069 -0.029335\n",
       "jupiter   -0.021008 -0.020747 -0.021562 -0.020974  -0.019874  0.099572\n",
       "saturn    -0.047606 -0.048266 -0.046910 -0.047589  -0.049660 -0.266256\n",
       "uran      -0.308761 -0.307478 -0.310030 -0.308342  -0.307212  0.240279\n",
       "neptun     0.652471  0.653585  0.651316  0.652486   0.653998  0.559193\n",
       "pluton     0.630388  0.631501  0.629232  0.630395   0.631941  0.559440\n",
       "Open       0.000000  0.999932  0.999870  0.999832   0.999827  0.360963\n",
       "High       0.999932  0.000000  0.999844  0.999918   0.999916  0.364143\n",
       "Low        0.999870  0.999844  0.000000  0.999896   0.999889  0.356183\n",
       "Close      0.999832  0.999918  0.999896  0.000000   0.999995  0.360351\n",
       "Adj Close  0.999827  0.999916  0.999889  0.999995   0.000000  0.361232\n",
       "Volume     0.360963  0.364143  0.356183  0.360351   0.361232  0.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open         0.630388\n",
       "High         0.631501\n",
       "Low          0.629232\n",
       "Close        0.630395\n",
       "Adj Close    0.631941\n",
       "Volume       0.559440\n",
       "Name: pluton, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_corr.loc['pluton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open         0.652471\n",
       "High         0.653585\n",
       "Low          0.651316\n",
       "Close        0.652486\n",
       "Adj Close    0.653998\n",
       "Volume       0.559193\n",
       "Name: neptun, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_corr.loc['neptun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_planets = df[['Date', 'pluton', 'neptun', 'Open', 'High', 'Low',\n",
    "       'Close', 'Adj Close', 'Volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>pluton</th>\n",
       "      <th>neptun</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981-01-05</td>\n",
       "      <td>204.212438</td>\n",
       "      <td>263.214863</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.150670</td>\n",
       "      <td>0.150670</td>\n",
       "      <td>0.118667</td>\n",
       "      <td>35728000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1981-01-19</td>\n",
       "      <td>204.330776</td>\n",
       "      <td>263.691294</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.147321</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.115590</td>\n",
       "      <td>41574400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1981-01-20</td>\n",
       "      <td>204.334748</td>\n",
       "      <td>263.723289</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.142299</td>\n",
       "      <td>0.112074</td>\n",
       "      <td>30083200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1981-01-21</td>\n",
       "      <td>204.338106</td>\n",
       "      <td>263.754959</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.146205</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.145089</td>\n",
       "      <td>0.114272</td>\n",
       "      <td>15904000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1981-01-22</td>\n",
       "      <td>204.340851</td>\n",
       "      <td>263.786297</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.147879</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.115590</td>\n",
       "      <td>35548800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7807</th>\n",
       "      <td>2011-12-23</td>\n",
       "      <td>277.019690</td>\n",
       "      <td>328.671539</td>\n",
       "      <td>14.274643</td>\n",
       "      <td>14.413929</td>\n",
       "      <td>14.267500</td>\n",
       "      <td>14.404643</td>\n",
       "      <td>12.426182</td>\n",
       "      <td>269399200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7808</th>\n",
       "      <td>2011-12-27</td>\n",
       "      <td>277.162639</td>\n",
       "      <td>328.769952</td>\n",
       "      <td>14.396428</td>\n",
       "      <td>14.610357</td>\n",
       "      <td>14.393572</td>\n",
       "      <td>14.518929</td>\n",
       "      <td>12.524765</td>\n",
       "      <td>265076000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7809</th>\n",
       "      <td>2011-12-28</td>\n",
       "      <td>277.198397</td>\n",
       "      <td>328.795638</td>\n",
       "      <td>14.531786</td>\n",
       "      <td>14.580358</td>\n",
       "      <td>14.333571</td>\n",
       "      <td>14.380000</td>\n",
       "      <td>12.404922</td>\n",
       "      <td>228662000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7810</th>\n",
       "      <td>2011-12-29</td>\n",
       "      <td>277.234148</td>\n",
       "      <td>328.821745</td>\n",
       "      <td>14.407143</td>\n",
       "      <td>14.487500</td>\n",
       "      <td>14.303928</td>\n",
       "      <td>14.468572</td>\n",
       "      <td>12.481326</td>\n",
       "      <td>215978000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7811</th>\n",
       "      <td>2011-12-30</td>\n",
       "      <td>277.269888</td>\n",
       "      <td>328.848270</td>\n",
       "      <td>14.411072</td>\n",
       "      <td>14.510000</td>\n",
       "      <td>14.410357</td>\n",
       "      <td>14.464286</td>\n",
       "      <td>12.477631</td>\n",
       "      <td>179662000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7812 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date      pluton      neptun       Open       High        Low  \\\n",
       "0     1981-01-05  204.212438  263.214863   0.151228   0.151228   0.150670   \n",
       "1     1981-01-19  204.330776  263.691294   0.146763   0.147321   0.146763   \n",
       "2     1981-01-20  204.334748  263.723289   0.142857   0.142857   0.142299   \n",
       "3     1981-01-21  204.338106  263.754959   0.145089   0.146205   0.145089   \n",
       "4     1981-01-22  204.340851  263.786297   0.146763   0.147879   0.146763   \n",
       "...          ...         ...         ...        ...        ...        ...   \n",
       "7807  2011-12-23  277.019690  328.671539  14.274643  14.413929  14.267500   \n",
       "7808  2011-12-27  277.162639  328.769952  14.396428  14.610357  14.393572   \n",
       "7809  2011-12-28  277.198397  328.795638  14.531786  14.580358  14.333571   \n",
       "7810  2011-12-29  277.234148  328.821745  14.407143  14.487500  14.303928   \n",
       "7811  2011-12-30  277.269888  328.848270  14.411072  14.510000  14.410357   \n",
       "\n",
       "          Close  Adj Close       Volume  \n",
       "0      0.150670   0.118667   35728000.0  \n",
       "1      0.146763   0.115590   41574400.0  \n",
       "2      0.142299   0.112074   30083200.0  \n",
       "3      0.145089   0.114272   15904000.0  \n",
       "4      0.146763   0.115590   35548800.0  \n",
       "...         ...        ...          ...  \n",
       "7807  14.404643  12.426182  269399200.0  \n",
       "7808  14.518929  12.524765  265076000.0  \n",
       "7809  14.380000  12.404922  228662000.0  \n",
       "7810  14.468572  12.481326  215978000.0  \n",
       "7811  14.464286  12.477631  179662000.0  \n",
       "\n",
       "[7812 rows x 9 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-0e8e6918032a>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  best_planets['difference'] = best_planets['Open'] - best_planets['Close']\n"
     ]
    }
   ],
   "source": [
    "best_planets['difference'] = best_planets['Open'] - best_planets['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7812 entries, 0 to 7811\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Date        7812 non-null   object \n",
      " 1   pluton      7812 non-null   float64\n",
      " 2   neptun      7812 non-null   float64\n",
      " 3   Open        7811 non-null   float64\n",
      " 4   High        7811 non-null   float64\n",
      " 5   Low         7811 non-null   float64\n",
      " 6   Close       7811 non-null   float64\n",
      " 7   Adj Close   7811 non-null   float64\n",
      " 8   Volume      7811 non-null   float64\n",
      " 9   difference  7811 non-null   float64\n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 610.4+ KB\n"
     ]
    }
   ],
   "source": [
    "best_planets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_planets = best_planets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = best_planets[['neptun', 'pluton']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = best_planets['difference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.5)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.5)\n",
    "\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024076846881196882"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, ridge.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without scaling\n",
    "\n",
    "**0.024076846881196882**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024074291360970442"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.5)\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "mean_absolute_error(y_test, ridge.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.024076846881196882 - 0.024074291360970442 == 0.000002555520226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04717873388804098"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, svr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04717873388804098"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfreg = RandomForestRegressor()\n",
    "\n",
    "rfreg.fit(X_train, y_train)\n",
    "\n",
    "mean_absolute_error(y_test, svr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (2.4.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.19.4)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six~=1.15.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/makarbaderko/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping,TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-01-27--1822'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datetime.now().strftime(\"%Y-%m-%d--%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'logs'\n",
    "\n",
    "board = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=30, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=15, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "196/196 [==============================] - 1s 2ms/step - loss: 0.3344 - val_loss: 0.0381\n",
      "Epoch 2/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0517 - val_loss: 0.0250\n",
      "Epoch 3/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0332 - val_loss: 0.0242\n",
      "Epoch 4/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0290 - val_loss: 0.0241\n",
      "Epoch 5/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0286 - val_loss: 0.0241\n",
      "Epoch 6/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0282 - val_loss: 0.0240\n",
      "Epoch 7/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0276 - val_loss: 0.0240\n",
      "Epoch 8/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0261 - val_loss: 0.0240\n",
      "Epoch 9/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0250 - val_loss: 0.0240\n",
      "Epoch 10/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0279 - val_loss: 0.0240\n",
      "Epoch 11/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0252 - val_loss: 0.0240\n",
      "Epoch 12/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0253 - val_loss: 0.0240\n",
      "Epoch 13/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0261 - val_loss: 0.0240\n",
      "Epoch 14/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0272 - val_loss: 0.0240\n",
      "Epoch 15/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0244 - val_loss: 0.0240\n",
      "Epoch 16/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0240 - val_loss: 0.0240\n",
      "Epoch 17/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0258 - val_loss: 0.0240\n",
      "Epoch 18/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0245 - val_loss: 0.0240\n",
      "Epoch 19/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0250 - val_loss: 0.0240\n",
      "Epoch 20/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0247 - val_loss: 0.0240\n",
      "Epoch 21/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0260 - val_loss: 0.0240\n",
      "Epoch 22/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0251 - val_loss: 0.0240\n",
      "Epoch 23/600\n",
      "196/196 [==============================] - 0s 997us/step - loss: 0.0261 - val_loss: 0.0240\n",
      "Epoch 24/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0242 - val_loss: 0.0240\n",
      "Epoch 25/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0242 - val_loss: 0.0240\n",
      "Epoch 26/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0251 - val_loss: 0.0240\n",
      "Epoch 27/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0243 - val_loss: 0.0240\n",
      "Epoch 28/600\n",
      "196/196 [==============================] - 0s 987us/step - loss: 0.0246 - val_loss: 0.0240\n",
      "Epoch 29/600\n",
      "196/196 [==============================] - 0s 994us/step - loss: 0.0244 - val_loss: 0.0240\n",
      "Epoch 30/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0239 - val_loss: 0.0240\n",
      "Epoch 31/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0256 - val_loss: 0.0240\n",
      "Epoch 32/600\n",
      "196/196 [==============================] - 0s 990us/step - loss: 0.0239 - val_loss: 0.0240\n",
      "Epoch 33/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0241 - val_loss: 0.0240\n",
      "Epoch 34/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0231 - val_loss: 0.0240\n",
      "Epoch 35/600\n",
      "196/196 [==============================] - 0s 985us/step - loss: 0.0234 - val_loss: 0.0240\n",
      "Epoch 36/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0242 - val_loss: 0.0240\n",
      "Epoch 37/600\n",
      "196/196 [==============================] - 0s 969us/step - loss: 0.0254 - val_loss: 0.0240\n",
      "Epoch 38/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0249 - val_loss: 0.0240\n",
      "Epoch 39/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0234 - val_loss: 0.0240\n",
      "Epoch 40/600\n",
      "196/196 [==============================] - 0s 998us/step - loss: 0.0245 - val_loss: 0.0240\n",
      "Epoch 41/600\n",
      "196/196 [==============================] - 0s 989us/step - loss: 0.0232 - val_loss: 0.0240\n",
      "Epoch 42/600\n",
      "196/196 [==============================] - 0s 979us/step - loss: 0.0218 - val_loss: 0.0240\n",
      "Epoch 43/600\n",
      "196/196 [==============================] - 0s 987us/step - loss: 0.0219 - val_loss: 0.0240\n",
      "Epoch 44/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0240\n",
      "Epoch 45/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 46/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 0.0240\n",
      "Epoch 00046: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb316a0c760>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1,\n",
    "          callbacks=[early_stop,board]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024044337172120546"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=30, activation='relu'))\n",
    "model.add(Dense(units=15, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "196/196 [==============================] - 1s 2ms/step - loss: 0.4011 - val_loss: 0.0730\n",
      "Epoch 2/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0519 - val_loss: 0.0324\n",
      "Epoch 3/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0277 - val_loss: 0.0273\n",
      "Epoch 4/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0243 - val_loss: 0.0257\n",
      "Epoch 5/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0251\n",
      "Epoch 6/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0233 - val_loss: 0.0247\n",
      "Epoch 7/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0245\n",
      "Epoch 8/600\n",
      "196/196 [==============================] - 0s 964us/step - loss: 0.0229 - val_loss: 0.0244\n",
      "Epoch 9/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0243\n",
      "Epoch 10/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 11/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0242\n",
      "Epoch 12/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0242\n",
      "Epoch 13/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0242\n",
      "Epoch 14/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0241\n",
      "Epoch 15/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0233 - val_loss: 0.0241\n",
      "Epoch 16/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 17/600\n",
      "196/196 [==============================] - 0s 980us/step - loss: 0.0218 - val_loss: 0.0241\n",
      "Epoch 18/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0241\n",
      "Epoch 19/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0241\n",
      "Epoch 20/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0241\n",
      "Epoch 21/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 0.0241\n",
      "Epoch 22/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0241\n",
      "Epoch 23/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0241\n",
      "Epoch 24/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0235 - val_loss: 0.0241\n",
      "Epoch 25/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0241\n",
      "Epoch 26/600\n",
      "196/196 [==============================] - 0s 996us/step - loss: 0.0233 - val_loss: 0.0241\n",
      "Epoch 27/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 28/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 29/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 30/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 31/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 0.0241\n",
      "Epoch 32/600\n",
      "196/196 [==============================] - 0s 991us/step - loss: 0.0243 - val_loss: 0.0241\n",
      "Epoch 33/600\n",
      "196/196 [==============================] - 0s 979us/step - loss: 0.0227 - val_loss: 0.0241\n",
      "Epoch 34/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0229 - val_loss: 0.0241\n",
      "Epoch 35/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0237 - val_loss: 0.0241\n",
      "Epoch 36/600\n",
      "196/196 [==============================] - 0s 975us/step - loss: 0.0220 - val_loss: 0.0241\n",
      "Epoch 37/600\n",
      "196/196 [==============================] - 0s 951us/step - loss: 0.0220 - val_loss: 0.0241\n",
      "Epoch 38/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0241\n",
      "Epoch 39/600\n",
      "196/196 [==============================] - 0s 993us/step - loss: 0.0227 - val_loss: 0.0241\n",
      "Epoch 40/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0241\n",
      "Epoch 41/600\n",
      "196/196 [==============================] - 0s 978us/step - loss: 0.0226 - val_loss: 0.0241\n",
      "Epoch 42/600\n",
      "196/196 [==============================] - 0s 974us/step - loss: 0.0228 - val_loss: 0.0241\n",
      "Epoch 43/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 44/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0240\n",
      "Epoch 45/600\n",
      "196/196 [==============================] - 0s 972us/step - loss: 0.0230 - val_loss: 0.0240\n",
      "Epoch 46/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.0240\n",
      "Epoch 47/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0240\n",
      "Epoch 48/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0240\n",
      "Epoch 49/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 50/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0240\n",
      "Epoch 51/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 0.0240\n",
      "Epoch 52/600\n",
      "196/196 [==============================] - 0s 988us/step - loss: 0.0221 - val_loss: 0.0240\n",
      "Epoch 53/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 54/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0240\n",
      "Epoch 55/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 0.0240\n",
      "Epoch 56/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 57/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0240\n",
      "Epoch 58/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0229 - val_loss: 0.0240\n",
      "Epoch 59/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.0240\n",
      "Epoch 60/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0240\n",
      "Epoch 61/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 62/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 63/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0240\n",
      "Epoch 64/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0240\n",
      "Epoch 65/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0240\n",
      "Epoch 66/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0240\n",
      "Epoch 67/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 68/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 69/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.0240\n",
      "Epoch 70/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 71/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0240\n",
      "Epoch 72/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0235 - val_loss: 0.0240\n",
      "Epoch 73/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0231 - val_loss: 0.0240\n",
      "Epoch 74/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0240\n",
      "Epoch 75/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0231 - val_loss: 0.0240\n",
      "Epoch 76/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0240\n",
      "Epoch 77/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 78/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0240\n",
      "Epoch 79/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0240\n",
      "Epoch 80/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0240\n",
      "Epoch 81/600\n",
      "196/196 [==============================] - 0s 993us/step - loss: 0.0231 - val_loss: 0.0240\n",
      "Epoch 82/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0233 - val_loss: 0.0240\n",
      "Epoch 83/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0237 - val_loss: 0.0240\n",
      "Epoch 84/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0211 - val_loss: 0.0240\n",
      "Epoch 85/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0240\n",
      "Epoch 86/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0240\n",
      "Epoch 87/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0240\n",
      "Epoch 88/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0240\n",
      "Epoch 89/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.0240\n",
      "Epoch 90/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0240\n",
      "Epoch 91/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0240\n",
      "Epoch 92/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0229 - val_loss: 0.0240\n",
      "Epoch 93/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0212 - val_loss: 0.0240\n",
      "Epoch 94/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 95/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 96/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0240\n",
      "Epoch 97/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 98/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0236 - val_loss: 0.0240\n",
      "Epoch 99/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0240\n",
      "Epoch 100/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0240\n",
      "Epoch 101/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 102/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0240\n",
      "Epoch 103/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0240\n",
      "Epoch 104/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0240\n",
      "Epoch 105/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0240\n",
      "Epoch 106/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.0240\n",
      "Epoch 107/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 108/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 0.0240\n",
      "Epoch 109/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0240\n",
      "Epoch 110/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0240\n",
      "Epoch 111/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0240\n",
      "Epoch 112/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0240\n",
      "Epoch 113/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.0240\n",
      "Epoch 114/600\n",
      "196/196 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 115/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0240\n",
      "Epoch 116/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 117/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0240\n",
      "Epoch 118/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 119/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0240\n",
      "Epoch 120/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0229 - val_loss: 0.0240\n",
      "Epoch 121/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0236 - val_loss: 0.0240\n",
      "Epoch 122/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0234 - val_loss: 0.0240\n",
      "Epoch 123/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 0.0240\n",
      "Epoch 124/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 125/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0240\n",
      "Epoch 126/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0240\n",
      "Epoch 127/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0240\n",
      "Epoch 128/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 129/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0240\n",
      "Epoch 130/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0240\n",
      "Epoch 131/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 132/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 0.0240\n",
      "Epoch 133/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 0.0240\n",
      "Epoch 134/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0240\n",
      "Epoch 135/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0240\n",
      "Epoch 136/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0240\n",
      "Epoch 137/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0240\n",
      "Epoch 138/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0240\n",
      "Epoch 139/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0209 - val_loss: 0.0240\n",
      "Epoch 140/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 141/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0231 - val_loss: 0.0240\n",
      "Epoch 142/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 0.0240\n",
      "Epoch 143/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 144/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 145/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0240\n",
      "Epoch 146/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0236 - val_loss: 0.0240\n",
      "Epoch 147/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0240\n",
      "Epoch 148/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0240\n",
      "Epoch 149/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0232 - val_loss: 0.0240\n",
      "Epoch 150/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 151/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0225 - val_loss: 0.0240\n",
      "Epoch 152/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.0240\n",
      "Epoch 153/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.0240\n",
      "Epoch 154/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0229 - val_loss: 0.0240\n",
      "Epoch 155/600\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 0.0240\n",
      "Epoch 00155: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb317c6fca0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1,\n",
    "          callbacks=[early_stop,board]\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024044337172120546"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reds = model.predict(X_test)\n",
    "mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing TensorFlow\n",
      "TF version: 2.4.0\n",
      "Installing Tensorflow Model Analysis and Dependencies\n",
      "\u001b[31mERROR: tensorflow 2.3.2 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.4 which is incompatible.\u001b[0m\n",
      "Beam version: 2.27.0\n",
      "TFMA version: 0.26.0\n"
     ]
    }
   ],
   "source": [
    "print('Installing TensorFlow')\n",
    "import tensorflow as tf\n",
    "print('TF version: {}'.format(tf.__version__))\n",
    "\n",
    "print('Installing Tensorflow Model Analysis and Dependencies')\n",
    "!pip install -q tensorflow_model_analysis\n",
    "import apache_beam as beam\n",
    "print('Beam version: {}'.format(beam.__version__))\n",
    "import tensorflow_model_analysis as tfma\n",
    "print('TFMA version: {}'.format(tfma.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/makarbaderko/Documents/Programming/freelance/a_kiselev/Astro_Sex'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_0,02.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model_0,02.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
